{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded POD: 459 rows\n",
      "Loaded NAV: 366 rows\n"
     ]
    }
   ],
   "source": [
    "# Enhanced POD Processing - Clean Version\n",
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "# Build Supabase engine\n",
    "DATABASE_DSN = (\n",
    "    \"postgresql://postgres.avcznjglmqhmzqtsrlfg:Czheyuan0227@\"\n",
    "    \"aws-0-us-east-2.pooler.supabase.com:6543/postgres?sslmode=require\"\n",
    ")\n",
    "engine = create_engine(DATABASE_DSN, pool_pre_ping=True)\n",
    "\n",
    "replace = pd.read_csv(\"item name replace.csv\")\n",
    "\n",
    "# Load and process POD data\n",
    "pod = pd.read_csv(\"open purchase orders.csv\", encoding=\"utf-8\", encoding_errors=\"replace\")\n",
    "pod.drop(columns=['Name', 'Amount', 'Open Balance', \"Rcv'd\", \"Memo\"], inplace=True)\n",
    "pod.rename(columns={\"Date\": \"Order Date\", \"Num\": \"QB Num\", \"Source Name\": \"Name\", \"Backordered\": \"Qty(+)\"}, inplace=True)\n",
    "pod.drop(pod.columns[0], axis=1, inplace=True)\n",
    "pod.dropna(how='all', inplace=True)\n",
    "pod.dropna(thresh=5, inplace=True)\n",
    "pod['Item'] = pod['Item'].str.split(':').str[1]\n",
    "pod = pod[pod['Item'] != 'Engineer Service- COS']\n",
    "pod = pod[pod['Item'] != 'RMA Services']\n",
    "# pod['Item'] = pod['Item'].replace(QB_mappings)\n",
    "pod['QB Num'] = pod['QB Num'].str.split('(').str[0]\n",
    "for col in ['Order Date', 'Deliv Date']:\n",
    "    pod[col] = pd.to_datetime(pod[col]).dt.strftime('%Y/%m/%d')\n",
    "pod.to_csv('open purchase2.csv',index=False)\n",
    "\n",
    "# Load and process NAV data\n",
    "nav = pd.read_csv(\"Sales Date return platform.csv\", usecols=['Document No.', \"Customer PO No.\", \"Customer Ordering Model\",\n",
    "                                                             \"OP Estimated Shipping Date\", \"Quantity\", \"No.\", \"External Document No.\",\n",
    "                                                             \"Customer Ordering Desc.\"], encoding='utf-8')\n",
    "nav.rename(columns={\"Customer PO No.\": \"QB Num\", \"Customer Ordering Model\": \"Item\", 'Document No.': \"Remark\",\n",
    "                    \"OP Estimated Shipping Date\": \"Ship Date\", \"Quantity\": \"Qty(+)\"}, inplace=True)\n",
    "nav = nav[nav['Item'] != 'Engineer Service- COS']\n",
    "nav = nav[nav['Item'] != 'CUSTOMER SERVICES']\n",
    "nav = nav[nav['Item'] != 'FORWARDING CHARGE, EXCLUDING IMPORT DUTY.']\n",
    "nav['QB Num'] = nav['QB Num'].str.split('(').str[0]\n",
    "\n",
    "print(f\"Loaded POD: {len(pod)} rows\")\n",
    "print(f\"Loaded NAV: {len(nav)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPANDING PRE-INSTALLED ITEMS ===\n",
      "Pre-installed items: 72\n",
      "Barebone items: 294\n",
      "Expanded to: 557 total rows\n",
      "Pre: 263\n",
      "Bare: 294\n"
     ]
    }
   ],
   "source": [
    "# Expand Pre-installed items into components\n",
    "print(\"=== EXPANDING PRE-INSTALLED ITEMS ===\")\n",
    "\n",
    "# Classify NAV items\n",
    "Pre_NAV = nav[nav['No.'].astype(str).str.startswith('S', na=False)]\n",
    "Bare_NAV = nav[~nav['No.'].astype(str).str.startswith('S', na=False)]\n",
    "\n",
    "Bare_NAV.to_csv('Barebone_NAV.csv', index=False)\n",
    "\n",
    "print(f\"Pre-installed items: {len(Pre_NAV)}\")\n",
    "print(f\"Barebone items: {len(Bare_NAV)}\")\n",
    "\n",
    "# Expand Pre-installed items\n",
    "def explode_pre_nav(pre_nav_df):\n",
    "    rows = []\n",
    "    for _, rec in pre_nav_df.fillna('').iterrows():\n",
    "        desc = str(rec.get('Customer Ordering Desc.', '')).replace('\\u00A0',' ').replace('\\u3000',' ').strip()\n",
    "        parts = re.split(r',\\s*including\\s*', desc, maxsplit=1, flags=re.I)\n",
    "        base = parts[0].split(',', 1)[0].strip()\n",
    "        comps = [c.strip() for c in parts[1].split(',')] if len(parts) > 1 else []\n",
    "        \n",
    "        targets = list(dict.fromkeys([*comps, base]))  # Remove duplicates, preserve order\n",
    "        \n",
    "        for t in targets:\n",
    "            new = rec.copy()\n",
    "            new['Customer Ordering Desc.'] = t\n",
    "            compact = t.replace(' ', '')\n",
    "            m = re.match(r'^(\\d+)x(.+)$', compact, flags=re.I)\n",
    "            if m:\n",
    "                mult = int(m.group(1))\n",
    "                new['Item'] = m.group(2)\n",
    "                qty = pd.to_numeric(new.get('Qty(+)'), errors='coerce')\n",
    "                new['Qty(+)'] = (float(qty) if pd.notna(qty) else 1.0) * mult\n",
    "            else:\n",
    "                new['Item'] = t\n",
    "            rows.append(new)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Expand Pre-installed items and combine with Barebone\n",
    "Expanded_Pre = explode_pre_nav(Pre_NAV)\n",
    "Expanded_Pre['Pre/Bare'] = 'Pre'\n",
    "Bare_NAV_labeled = Bare_NAV.copy()\n",
    "Bare_NAV_labeled['Pre/Bare'] = 'Bare'\n",
    "\n",
    "Final_NAV = pd.concat([Expanded_Pre, Bare_NAV_labeled], ignore_index=True)\n",
    "replace_dict = dict(zip(replace['NAV'], replace['QB']))\n",
    "Final_NAV['Item'] = Final_NAV['Item'].replace(replace_dict)\n",
    "Final_NAV.to_csv('Final_NAV.csv', index=False)\n",
    "\n",
    "print(f\"Expanded to: {len(Final_NAV)} total rows\")\n",
    "print(f\"Pre: {len(Final_NAV[Final_NAV['Pre/Bare'] == 'Pre'])}\")\n",
    "print(f\"Bare: {len(Final_NAV[Final_NAV['Pre/Bare'] == 'Bare'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SO \n",
    "#BackorderedÊòØÊåáÁõÆÂâçSOÂ∞öÊú™Âá∫Ë≤®ÁöÑÊï∏ÈáèÔºåÂ¶ÇÊûúÁî®QTYÁöÑË©±ÊúÉÂåÖÂê´Â∑≤Á∂ìpartialÂá∫Ë≤®ÁöÑ\n",
    "#supabseÈáåÂ∑≤Áªè\"Backordered\" ‚Äî‚Äî‚Äî„Äã\"Qty\"\n",
    "# SO = pd.read_csv(\"Data/open sales orders.csv\", encoding=\"utf-8\", encoding_errors=\"ignore\")\n",
    "SO = pd.read_sql_table(\"open_sales_orders\", con=engine, schema=\"public\") # Pull from Supabase\n",
    "SO.rename(columns={\"Date\":\"Order Date\",\"Num\":\"QB Num\",\"Qty\":\"Qty(-)\"},inplace=True)\n",
    "SO = SO.drop(SO.columns[[0]], axis =1)\n",
    "SO = SO.drop(columns=['Type','Due Date','Terms','Amount','Deliv Date','Open Balance',\"Invoiced\",\"Rep\"], axis =1)\n",
    "SO = SO.dropna(axis=0, how='all',subset=None, inplace=False)\n",
    "SO = SO.dropna(thresh=6)\n",
    "SO['Qty(+)']=\"0\"\n",
    "SO['Remark']=\"\"\n",
    "SO['Order Date']= pd.to_datetime(SO['Order Date'])\n",
    "SO['Order Date'] = SO['Order Date'].dt.strftime('%Y/%m/%d')\n",
    "SO['Ship Date']= pd.to_datetime(SO['Ship Date'])\n",
    "SO['Ship Date'] = SO['Ship Date'].dt.strftime('%Y/%m/%d')\n",
    "SO['Pre/Bare'] = \"Out\"\n",
    "\n",
    "\n",
    "\n",
    "#Inventory\n",
    "INV = pd.read_sql_table(\"inventory_status\", con=engine, schema=\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ËÆÄÂèñ open purchase2.csv ‰∏¶ËôïÁêÜÊï∏Êìö\n",
    "a = pd.read_csv('open purchase2.csv', usecols=['QB Num', \"Order Date\", \"Inventory Site\", \"P. O. #\", \"Name\", \"Item\"])\n",
    "a.drop_duplicates(inplace=True)\n",
    "a['Qty(-)'] = \"0\"\n",
    "\n",
    "fil = set(a['Item'])\n",
    "Final_NAV = Final_NAV[Final_NAV['Item'].isin(fil)]\n",
    "a = a.drop(columns=[\"Item\"])\n",
    "a.drop_duplicates(inplace=True)\n",
    "\n",
    "# Âêà‰Ωµ NAV Âíå open purchase2.csv\n",
    "Final = pd.merge(left=Final_NAV, right=a, on=[\"QB Num\"], how=\"left\")\n",
    "columns = ['Order Date', 'Ship Date', 'QB Num', \"P. O. #\", \"Name\", 'Qty(-)', 'Qty(+)', 'Item', 'Inventory Site', 'Remark', 'Pre/Bare']\n",
    "Final.to_csv('Final.csv', index=False, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "SONAV = pd.concat([SO,Final])\n",
    "mappings = {\n",
    "    'M.280-SSD-256GB-PCIe44-TLC5WT-T': 'M.280-SSD-256GB-PCIe44-TLC5WT-TD',\n",
    "    'M.280-SSD-512GB-PCIe44-TLC5WT-T': 'M.280-SSD-512GB-PCIe44-TLC5WT-TD',\n",
    "    'M.242-SSD-256GB-PCIe34-TLC5WT-T': 'M.242-SSD-256GB-PCIe34-TLC5WT-TD',\n",
    "    'M.242-SSD-512GB-PCIe34-TLC5WT-T': 'M.242-SSD-512GB-PCIe34-TLC5WT-TD',\n",
    "    'M.242-SSD-128GB-PCIe34-TLC5WT-T': 'M.242-SSD-128GB-PCIe34-TLC5WT-TD',\n",
    "    'Cblkit-FP-NRU-230V-AWP_NRU-240S': 'Cblkit-FP-NRU-230V-AWP_NRU-240S-AWP',\n",
    "}\n",
    "SONAV = SONAV.replace({'Item': mappings})\n",
    "SONAV = SONAV.sort_values(by=[\"Inventory Site\",\"Item\",\"Ship Date\"], ascending=False)\n",
    "SONAV.to_csv(\"SONAV.csv\", index=False,columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "\n",
    "def build_timephased_qty(\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    freq=\"D\",\n",
    "    initial_onhand_df=None,   # pass INV here\n",
    "    site_filter=\"WH01S-NTA\",  # focus on this site\n",
    "):\n",
    "    df = SONAV\n",
    "\n",
    "    # ---- Filter Site ----\n",
    "    df[\"Inventory Site\"] = df[\"Inventory Site\"].astype(str).str.strip()\n",
    "    if site_filter is not None:\n",
    "        df = df[df[\"Inventory Site\"] == site_filter].copy()\n",
    "\n",
    "    # clean numeric & date\n",
    "    df[\"Qty(+)\"] = pd.to_numeric(df[\"Qty(+)\"], errors=\"coerce\").fillna(0)\n",
    "    df[\"Qty(-)\"] = pd.to_numeric(df[\"Qty(-)\"], errors=\"coerce\").fillna(0)\n",
    "    df[\"Ship Date\"] = pd.to_datetime(df[\"Ship Date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Ship Date\"])\n",
    "    df[\"Item\"] = df[\"Item\"].astype(str).str.strip()\n",
    "\n",
    "    # net movement per date\n",
    "    df[\"qty_change\"] = df[\"Qty(+)\"] - df[\"Qty(-)\"]\n",
    "    events = (df.groupby([\"Inventory Site\",\"Item\",\"Ship Date\"], as_index=False)[\"qty_change\"]\n",
    "                .sum()\n",
    "                .sort_values([\"Inventory Site\",\"Item\",\"Ship Date\"]))\n",
    "\n",
    "    # horizon\n",
    "    if start_date is None:\n",
    "        start = pd.Timestamp(\"2025-01-01\")   # or use today().date()\n",
    "    else:\n",
    "        start = pd.to_datetime(start_date).normalize()\n",
    "\n",
    "    if end_date is None:\n",
    "        end = start + timedelta(days=30)\n",
    "    else:\n",
    "        end = pd.to_datetime(end_date).normalize()\n",
    "\n",
    "    date_index = pd.date_range(start, end, freq=freq)\n",
    "\n",
    "    # all (site,item) pairs from events\n",
    "    pairs = events[[\"Inventory Site\",\"Item\"]].drop_duplicates()\n",
    "\n",
    "    # build continuous timeline\n",
    "    all_rows = []\n",
    "    for (site, item), grp in events.groupby([\"Inventory Site\",\"Item\"]):\n",
    "        s = grp.set_index(\"Ship Date\")[\"qty_change\"].reindex(date_index, fill_value=0.0)\n",
    "        out = pd.DataFrame({\n",
    "            \"Ship Date\": date_index,\n",
    "            \"Inventory Site\": site,\n",
    "            \"Item\": item,\n",
    "            \"Net Movement\": s.values\n",
    "        })\n",
    "        all_rows.append(out)\n",
    "\n",
    "    timeline = pd.concat(all_rows, ignore_index=True) if all_rows else pd.DataFrame(\n",
    "        columns=[\"Ship Date\",\"Inventory Site\",\"Item\",\"Net Movement\"]\n",
    "    )\n",
    "\n",
    "    # ----- add initial on-hand (INV) -----\n",
    "    # Case: INV has NO site -> broadcast to the chosen site\n",
    "    if initial_onhand_df is not None and not initial_onhand_df.empty:\n",
    "        onhand = initial_onhand_df.copy()\n",
    "        # normalize columns from INV\n",
    "        if \"Part_Number\" in onhand.columns:\n",
    "            onhand = onhand.rename(columns={\"Part_Number\":\"Item\"})\n",
    "        onhand[\"Item\"] = onhand[\"Item\"].astype(str).str.strip()\n",
    "        onhand[\"On Hand\"] = pd.to_numeric(onhand[\"On Hand\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "        if \"Inventory Site\" not in onhand.columns:\n",
    "            onhand[\"Inventory Site\"] = site_filter  # broadcast INV to this site\n",
    "\n",
    "        onhand = onhand[[\"Inventory Site\",\"Item\",\"On Hand\"]]\n",
    "        # limit to items present in timeline to avoid bloat\n",
    "        if not timeline.empty:\n",
    "            onhand = onhand.merge(pairs, on=[\"Inventory Site\",\"Item\"], how=\"inner\")\n",
    "\n",
    "        timeline = timeline.merge(onhand, on=[\"Inventory Site\",\"Item\"], how=\"left\")\n",
    "        timeline[\"On Hand\"] = timeline[\"On Hand\"].fillna(0.0)\n",
    "    else:\n",
    "        timeline[\"On Hand\"] = 0.0\n",
    "\n",
    "    # running balance\n",
    "    if not timeline.empty:\n",
    "        timeline.sort_values([\"Inventory Site\",\"Item\",\"Ship Date\"], inplace=True)\n",
    "        cum_net = timeline.groupby([\"Inventory Site\",\"Item\"])[\"Net Movement\"].cumsum()\n",
    "        init_onhand = timeline.groupby([\"Inventory Site\",\"Item\"])[\"On Hand\"].transform(\"first\")\n",
    "        timeline[\"Projected Qty\"] = init_onhand + cum_net\n",
    "    else:\n",
    "        timeline[\"Projected Qty\"] = pd.Series(dtype=float)\n",
    "\n",
    "    return events, timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Ship Date  On Hand  Net Movement  Projected Qty\n",
      "15677 2025-10-09      9.0           0.0            9.0\n",
      "15678 2025-10-10      9.0           0.0            9.0\n",
      "15679 2025-10-11      9.0           0.0            9.0\n",
      "15680 2025-10-12      9.0           0.0            9.0\n",
      "15681 2025-10-13      9.0           0.0            9.0\n",
      "15682 2025-10-14      9.0           0.0            9.0\n",
      "15683 2025-10-15      9.0           0.0            9.0\n",
      "15684 2025-10-16      9.0           0.0            9.0\n",
      "15685 2025-10-17      9.0           0.0            9.0\n",
      "15686 2025-10-18      9.0           0.0            9.0\n",
      "15687 2025-10-19      9.0           0.0            9.0\n",
      "15688 2025-10-20      9.0           0.0            9.0\n",
      "15689 2025-10-21      9.0           0.0            9.0\n",
      "15690 2025-10-22      9.0           0.0            9.0\n",
      "15691 2025-10-23      9.0           0.0            9.0\n",
      "15692 2025-10-24      9.0           0.0            9.0\n",
      "15693 2025-10-25      9.0           0.0            9.0\n",
      "15694 2025-10-26      9.0           0.0            9.0\n",
      "15695 2025-10-27      9.0           0.0            9.0\n",
      "15696 2025-10-28      9.0           0.0            9.0\n",
      "15697 2025-10-29      9.0           0.0            9.0\n",
      "15698 2025-10-30      9.0           0.0            9.0\n",
      "15699 2025-10-31      9.0           0.0            9.0\n",
      "15700 2025-11-01      9.0           0.0            9.0\n",
      "15701 2025-11-02      9.0           0.0            9.0\n"
     ]
    }
   ],
   "source": [
    "# Prepare INV (no site column in your screenshot)\n",
    "initial_onhand_df = INV[[\"Part_Number\",\"On Hand\"]].copy()\n",
    "\n",
    "events, timeline = build_timephased_qty(\n",
    "    start_date=date.today(),\n",
    "    end_date=date.today() + timedelta(days=60),\n",
    "    freq=\"D\",\n",
    "    initial_onhand_df=initial_onhand_df,\n",
    "    site_filter=\"WH01S-NTA\",\n",
    ")\n",
    "\n",
    "part_view = timeline[ \n",
    "    (timeline[\"Item\"] == \"TB-10\") & (timeline[\"Inventory Site\"] == \"WH01S-NTA\")\n",
    "]\n",
    "print(part_view[[\"Ship Date\",\"On Hand\",\"Net Movement\",\"Projected Qty\"]].head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_qty = timeline[timeline['Projected Qty'] < 0]\n",
    "negative_qty.to_csv(\"negative_qty.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENHANCE POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENHANCED POD PROCESSING ===\n",
      "Processed 460 records\n",
      "Found 172 partial shipments\n"
     ]
    }
   ],
   "source": [
    "# Enhanced POD Processing with Pre/Bare Logic\n",
    "print(\"=== ENHANCED POD PROCESSING ===\")\n",
    "\n",
    "# Create lookup sets\n",
    "pre_items = set(Final_NAV[Final_NAV['Pre/Bare'] == 'Pre']['Item'].unique())\n",
    "bare_items = set(Final_NAV[Final_NAV['Pre/Bare'] == 'Bare']['Item'].unique())\n",
    "\n",
    "# Process each POD item\n",
    "enhanced_pod_list = []\n",
    "partial_shipments = []\n",
    "\n",
    "for _, pod_row in pod.iterrows():\n",
    "    item = pod_row['Item']\n",
    "    qb_num = pod_row['QB Num']\n",
    "    \n",
    "    # Classify item and get matches\n",
    "    if item in pre_items:\n",
    "        item_type = 'Pre'\n",
    "        nav_matches = Final_NAV[(Final_NAV['QB Num'] == qb_num) & (Final_NAV['Pre/Bare'] == 'Pre')]\n",
    "    elif item in bare_items:\n",
    "        item_type = 'Bare'\n",
    "        nav_matches = Final_NAV[(Final_NAV['Item'] == item) & (Final_NAV['QB Num'] == qb_num) & (Final_NAV['Pre/Bare'] == 'Bare')]\n",
    "    else:\n",
    "        item_type = 'Unknown'\n",
    "        nav_matches = pd.DataFrame()\n",
    "    \n",
    "    # Handle matches\n",
    "    if len(nav_matches) == 0:\n",
    "        new_row = pod_row.copy()\n",
    "        new_row['Pre/Bare'] = item_type\n",
    "        new_row['Ship Date'] = ''\n",
    "        new_row['Status'] = 'No NAV Data'\n",
    "        enhanced_pod_list.append(new_row)\n",
    "    elif len(nav_matches) == 1:\n",
    "        nav_row = nav_matches.iloc[0]\n",
    "        new_row = pod_row.copy()\n",
    "        new_row['Pre/Bare'] = item_type\n",
    "        new_row['Ship Date'] = nav_row['Ship Date']\n",
    "        new_row['Status'] = 'Complete'\n",
    "        enhanced_pod_list.append(new_row)\n",
    "    else:\n",
    "        # Multiple shipments\n",
    "        unique_ship_dates = nav_matches['Ship Date'].unique()\n",
    "        original_qty = pod_row['Qty(+)']\n",
    "        \n",
    "        for i, ship_date in enumerate(unique_ship_dates, 1):\n",
    "            new_row = pod_row.copy()\n",
    "            new_row['QB Num'] = f\"{qb_num}(P{i})\"\n",
    "            new_row['Pre/Bare'] = item_type\n",
    "            new_row['Ship Date'] = ship_date\n",
    "            new_row['Status'] = 'Partial'\n",
    "            new_row['Qty(+)'] = original_qty / len(unique_ship_dates)\n",
    "            enhanced_pod_list.append(new_row)\n",
    "        \n",
    "        partial_shipments.append({\n",
    "            'original_qb': qb_num,\n",
    "            'item': item,\n",
    "            'type': item_type,\n",
    "            'count': len(unique_ship_dates)\n",
    "        })\n",
    "\n",
    "print(f\"Processed {len(enhanced_pod_list)} records\")\n",
    "print(f\"Found {len(partial_shipments)} partial shipments\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL RESULTS ===\n",
      "Original POD: 459 rows\n",
      "Enhanced POD: 460 rows\n",
      "Records added: 1\n",
      "\n",
      "With Ship Dates: 432\n",
      "Without Ship Dates: 28\n",
      "Success Rate: 93.9%\n",
      "\n",
      "Status Distribution:\n",
      "Status\n",
      "Partial        173\n",
      "Complete       159\n",
      "No NAV Data    128\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Pre/Bare Distribution:\n",
      "Pre/Bare\n",
      "Pre        193\n",
      "Bare       178\n",
      "Unknown     89\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 5 rows:\n",
      "        QB Num                             Item Pre/Bare  Ship Date  Qty(+)  \\\n",
      "3   POD-251046       AccsyBx-6AntiVG-POC-551VTC  Unknown               34.0   \n",
      "4   POD-251047       AccsyBx-6AntiVG-POC-551VTC  Unknown               24.0   \n",
      "5   POD-251048       AccsyBx-6AntiVG-POC-551VTC  Unknown               29.0   \n",
      "8   POD-251338  AccsyBx-Cardholder-9160GC-2000E     Bare  2025/10/8     1.0   \n",
      "11  POD-251279              AccsyBx-FAN-NRU-100     Bare  2025/10/8     8.0   \n",
      "\n",
      "   Inventory Site      P. O. #                        Name  Order Date  \\\n",
      "3       Drop Ship  SO-20251050  Neousys Technology Incorp.  2025/07/24   \n",
      "4       Drop Ship  SO-20251051  Neousys Technology Incorp.  2025/07/24   \n",
      "5       Drop Ship  SO-20251052  Neousys Technology Incorp.  2025/07/24   \n",
      "8       WH01S-NTA  SO-20251316  Neousys Technology Incorp.  2025/09/18   \n",
      "11      WH01S-NTA  SO-20251262  Neousys Technology Incorp.  2025/09/08   \n",
      "\n",
      "         Status  \n",
      "3   No NAV Data  \n",
      "4   No NAV Data  \n",
      "5   No NAV Data  \n",
      "8      Complete  \n",
      "11     Complete  \n"
     ]
    }
   ],
   "source": [
    "# Create final enhanced POD DataFrame\n",
    "enhanced_pod_final = pd.DataFrame(enhanced_pod_list)\n",
    "\n",
    "# Reorder columns\n",
    "column_order = ['QB Num', 'Item', 'Pre/Bare', 'Ship Date', 'Qty(+)', 'Order Date', 'Status']\n",
    "for col in ['Inventory Site', 'P. O. #', 'Name']:\n",
    "    if col in enhanced_pod_final.columns:\n",
    "        column_order.insert(-2, col)\n",
    "\n",
    "existing_columns = [col for col in column_order if col in enhanced_pod_final.columns]\n",
    "enhanced_pod_final = enhanced_pod_final[existing_columns]\n",
    "\n",
    "# Display results\n",
    "print(\"=== FINAL RESULTS ===\")\n",
    "print(f\"Original POD: {len(pod)} rows\")\n",
    "print(f\"Enhanced POD: {len(enhanced_pod_final)} rows\")\n",
    "print(f\"Records added: {len(enhanced_pod_final) - len(pod)}\")\n",
    "print()\n",
    "\n",
    "got_ship_dates = enhanced_pod_final['Ship Date'].notna().sum()\n",
    "no_ship_dates = enhanced_pod_final['Ship Date'].isna().sum()\n",
    "print(f\"With Ship Dates: {got_ship_dates}\")\n",
    "print(f\"Without Ship Dates: {no_ship_dates}\")\n",
    "print(f\"Success Rate: {(got_ship_dates / len(enhanced_pod_final) * 100):.1f}%\")\n",
    "print()\n",
    "\n",
    "print(\"Status Distribution:\")\n",
    "print(enhanced_pod_final['Status'].value_counts())\n",
    "print()\n",
    "\n",
    "print(\"Pre/Bare Distribution:\")\n",
    "print(enhanced_pod_final['Pre/Bare'].value_counts())\n",
    "print()\n",
    "\n",
    "print(\"First 5 rows:\")\n",
    "print(enhanced_pod_final.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PRE/BARE DISTRIBUTION ANALYSIS ===\n",
      "\n",
      "1. NAV PRE/BARE DISTRIBUTION:\n",
      "   Total NAV rows: 223\n",
      "   Pre-installed rows: 37\n",
      "   Barebone rows: 186\n",
      "\n",
      "   Pre-installed unique items: 23\n",
      "   Barebone unique items: 118\n",
      "\n",
      "2. POD PRE/BARE DISTRIBUTION:\n",
      "   Total POD rows: 459\n",
      "   Pre-installed rows: 9\n",
      "   Barebone rows: 224\n",
      "   Unknown/No NAV match: 228\n",
      "\n",
      "   Pre-installed unique items: 5\n",
      "   Barebone unique items: 113\n",
      "   Unknown unique items: 126\n",
      "\n",
      "3. SUMMARY:\n",
      "   NAV: 37 Pre + 186 Bare = 223 total\n",
      "   POD: 9 Pre + 224 Bare + 228 Unknown = 459 total\n",
      "\n",
      "4. ITEM OVERLAP:\n",
      "   Common Pre items (in both NAV and POD): 5\n",
      "   Common Bare items (in both NAV and POD): 113\n"
     ]
    }
   ],
   "source": [
    "# Pre/Bare Distribution Analysis Function\n",
    "def analyze_pre_bare_distribution():\n",
    "    \"\"\"\n",
    "    Analyze Pre/Bare distribution in both NAV and POD datasets\n",
    "    \"\"\"\n",
    "    print('=== PRE/BARE DISTRIBUTION ANALYSIS ===')\n",
    "    print()\n",
    "    \n",
    "    # NAV Analysis\n",
    "    print('1. NAV PRE/BARE DISTRIBUTION:')\n",
    "    Pre_NAV = nav[nav['No.'].astype(str).str.startswith('S', na=False)]\n",
    "    Bare_NAV = nav[~nav['No.'].astype(str).str.startswith('S', na=False)]\n",
    "    \n",
    "    print(f'   Total NAV rows: {len(nav)}')\n",
    "    print(f'   Pre-installed rows: {len(Pre_NAV)}')\n",
    "    print(f'   Barebone rows: {len(Bare_NAV)}')\n",
    "    print()\n",
    "    \n",
    "    # NAV unique items\n",
    "    pre_items_nav = set(Pre_NAV['Item'].unique())\n",
    "    bare_items_nav = set(Bare_NAV['Item'].unique())\n",
    "    print(f'   Pre-installed unique items: {len(pre_items_nav)}')\n",
    "    print(f'   Barebone unique items: {len(bare_items_nav)}')\n",
    "    print()\n",
    "    \n",
    "    # POD Analysis - classify based on what exists in NAV\n",
    "    print('2. POD PRE/BARE DISTRIBUTION:')\n",
    "    pod_pre_items = pod[pod['Item'].isin(pre_items_nav)]\n",
    "    pod_bare_items = pod[pod['Item'].isin(bare_items_nav)]\n",
    "    pod_unknown_items = pod[~pod['Item'].isin(pre_items_nav | bare_items_nav)]\n",
    "    \n",
    "    print(f'   Total POD rows: {len(pod)}')\n",
    "    print(f'   Pre-installed rows: {len(pod_pre_items)}')\n",
    "    print(f'   Barebone rows: {len(pod_bare_items)}')\n",
    "    print(f'   Unknown/No NAV match: {len(pod_unknown_items)}')\n",
    "    print()\n",
    "    \n",
    "    # POD unique items\n",
    "    print(f'   Pre-installed unique items: {len(set(pod_pre_items[\"Item\"].unique()))}')\n",
    "    print(f'   Barebone unique items: {len(set(pod_bare_items[\"Item\"].unique()))}')\n",
    "    print(f'   Unknown unique items: {len(set(pod_unknown_items[\"Item\"].unique()))}')\n",
    "    print()\n",
    "    \n",
    "    # Summary\n",
    "    print('3. SUMMARY:')\n",
    "    print(f'   NAV: {len(Pre_NAV)} Pre + {len(Bare_NAV)} Bare = {len(nav)} total')\n",
    "    print(f'   POD: {len(pod_pre_items)} Pre + {len(pod_bare_items)} Bare + {len(pod_unknown_items)} Unknown = {len(pod)} total')\n",
    "    print()\n",
    "    \n",
    "    # Check overlap\n",
    "    print('4. ITEM OVERLAP:')\n",
    "    common_pre_items = pre_items_nav.intersection(set(pod['Item'].unique()))\n",
    "    common_bare_items = bare_items_nav.intersection(set(pod['Item'].unique()))\n",
    "    print(f'   Common Pre items (in both NAV and POD): {len(common_pre_items)}')\n",
    "    print(f'   Common Bare items (in both NAV and POD): {len(common_bare_items)}')\n",
    "    \n",
    "    return {\n",
    "        'nav': {'total': len(nav), 'pre': len(Pre_NAV), 'bare': len(Bare_NAV)},\n",
    "        'pod': {'total': len(pod), 'pre': len(pod_pre_items), 'bare': len(pod_bare_items), 'unknown': len(pod_unknown_items)},\n",
    "        'overlap': {'pre': len(common_pre_items), 'bare': len(common_bare_items)}\n",
    "    }, bare_items_nav, common_bare_items\n",
    "\n",
    "# Run the analysis\n",
    "distribution_stats, bare_items_nav, common_bare_items = analyze_pre_bare_distribution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common: 113\n",
      "Only in NAV: 5\n",
      "Only in POD: 130\n",
      "Uncommon (symmetric diff): 135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AccsyBx-Cardholder-9160GC-2000EAda',\n",
       " 'Cbl-M12A17M-VGA-180CM3',\n",
       " 'Cbl-M12A8M-2DB9M_OW2-180CM1',\n",
       " 'PA-280W-CW6P-2P-1',\n",
       " 'RGS-8805GC'}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize\n",
    "nav_items = set(Bare_NAV['Item'].unique())\n",
    "pod_items = set(pod['Item'].unique())\n",
    "\n",
    "# Common + uncommon\n",
    "common_bare_items = nav_items & pod_items\n",
    "only_in_nav = nav_items - pod_items          # items in NAV Bare, not in POD\n",
    "only_in_pod = pod_items - nav_items          # items in POD, not in NAV\n",
    "uncommon = nav_items ^ pod_items             # in exactly one side\n",
    "\n",
    "print(f\"Common: {len(common_bare_items)}\")\n",
    "print(f\"Only in NAV: {len(only_in_nav)}\")\n",
    "print(f\"Only in POD: {len(only_in_pod)}\")\n",
    "print(f\"Uncommon (symmetric diff): {len(uncommon)}\")\n",
    "\n",
    "only_in_nav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SHIP DATE VALIDATION ===\n",
      "\n",
      "Loaded NTA Shipping Schedule: 404 rows\n",
      "Columns: ['Date', 'SO NO.', 'Ship to', 'Customer PO No.', 'Reference', 'Project Code', 'Model Name', 'Assemble Option', 'Qty', 'Remark', 'Ship Date', 'Description']\n",
      "\n",
      "Sample NTA Shipping Schedule:\n",
      "        Date      SO NO.                           Ship to   Customer PO No.  \\\n",
      "0 2025-08-26  LO25080011  Neousys Technology America, Inc.  For NTA_CoastIPC   \n",
      "1 2025-07-28  SO25070447                  COAST Automation        POD-251046   \n",
      "2 2025-07-28  SO25070447                  COAST Automation        POD-251046   \n",
      "3 2025-09-03  SO25090068                  COAST Automation        POD-251253   \n",
      "4 2025-09-15  SO25090278                  COAST Automation        POD-251304   \n",
      "\n",
      "  Reference  Project Code                  Model Name Assemble Option  Qty  \\\n",
      "0       NaN           NaN                  RGS-8805GC              No    1   \n",
      "1    P96577           NaN        POC-551VTC-GLE150-FP              No   34   \n",
      "2    P96577           NaN  AccsyBx-6AntiVG-POC-551VTC              No   34   \n",
      "3   P96750Q           NaN                     POC-410              No    5   \n",
      "4    P96788           NaN         AccsyBx-FAN-POC-545              No    6   \n",
      "\n",
      "           Remark            Ship Date  \\\n",
      "0  TOÁîüÁÆ°:‰ΩøÁî®WH03Â∫´Â≠ò.  2025-09-10 00:00:00   \n",
      "1             NaN  2025-10-03 00:00:00   \n",
      "2             NaN  2025-10-03 00:00:00   \n",
      "3             NaN  2025-10-03 00:00:00   \n",
      "4             NaN  2025-10-03 00:00:00   \n",
      "\n",
      "                                         Description  \n",
      "0  Rugged and Compact AMD EPYC 7003/7002 \"MILAN/R...  \n",
      "1  AMD Ryzen‚Ñ¢ V1000 Ultra-compact In-vehicle Cont...  \n",
      "2  Accessory box kit including Anti-Vibrate Gromm...  \n",
      "3  POC-410 Intel¬Æ ElkhartLake ultra-compact embed...  \n",
      "4  Fan kit for POC-545 include fan, fan frame, fa...  \n",
      "\n",
      "=== COLUMN MAPPING ===\n",
      "Available columns: ['Date', 'SO NO.', 'Ship to', 'Customer PO No.', 'Reference', 'Project Code', 'Model Name', 'Assemble Option', 'Qty', 'Remark', 'Ship Date', 'Description']\n",
      "Identified columns:\n",
      "  QB Num column: Customer PO No.\n",
      "  Item column: Model Name\n",
      "  Ship Date column: Ship Date\n",
      "\n",
      "Column names standardized\n",
      "\n",
      "=== TESTING BARE ITEMS ===\n",
      "Testing 178 Bare items\n",
      "NOT FOUND - POD-251338 AccsyBx-Cardholder-9160GC-2000E: Enhanced=2025/10/8\n",
      "MISMATCH - POD-251383 Ant-RP_SMAM-WiFi-108MM: Enhanced=nan, NTA=2025-10-22 00:00:00\n",
      "MISMATCH - POD-251394 TB-3Pole-Push: Enhanced=nan, NTA=TBC\n",
      "NOT FOUND - POD-251244(P1) mPCIeHS-BTWifi-WT-6218: Enhanced=2025/10/29\n",
      "NOT FOUND - POD-251244(P2) mPCIeHS-BTWifi-WT-6218: Enhanced=2025/12/24\n",
      "MISMATCH - POD-251383 PCIe-PoE352at: Enhanced=nan, NTA=2025-10-22 00:00:00\n",
      "MISMATCH - POD-251371 PCIe-PoE550X: Enhanced=nan, NTA=2025-10-16 00:00:00\n",
      "MISMATCH - POD-251229 DDR4-32GB-ECC32WT-IK: Enhanced=nan, NTA=TBC\n",
      "MISMATCH - POD-251229 DDR4-32GB-ECC32WT-SM: Enhanced=nan, NTA=TBC\n",
      "MISMATCH - POD-251229 DDR4-32GB-ECC32WT-SP: Enhanced=nan, NTA=TBC\n",
      "MISMATCH - POD-251229 DDR4-32GB-ECC32WT-TD: Enhanced=nan, NTA=TBC\n",
      "MISMATCH - POD-251381 DDR5-8GB-56-SM: Enhanced=nan, NTA=2025-10-22 00:00:00\n",
      "MISMATCH - POD-251212 DDR5-8GB-56WT-SM: Enhanced=nan, NTA=TBC\n",
      "MISMATCH - POD-251383 MezIO-V20: Enhanced=nan, NTA=2025-10-22 00:00:00\n",
      "MISMATCH - POD-251211 DINRAIL-O: Enhanced=, NTA=2025-10-08 00:00:00\n",
      "MISMATCH - POD-251361 DINRAIL-O: Enhanced=, NTA=2025-10-17 00:00:00\n",
      "MISMATCH - POD-251327 Wmkit-H-POC300_400: Enhanced=, NTA=2025-11-07 00:00:00\n",
      "MISMATCH - POD-251385 Wmkit-NRU-50: Enhanced=nan, NTA=2025-10-22 00:00:00\n",
      "MISMATCH - POD-251378 Wmkit-POC465AWP: Enhanced=nan, NTA=TBC\n",
      "MISMATCH - POD-251379 Wmkit-POC465AWP: Enhanced=nan, NTA=TBC\n",
      "MISMATCH - POD-251327 Wmkit-V-POC300_400: Enhanced=, NTA=2025-11-07 00:00:00\n",
      "MISMATCH - POD-251382 AC-IMX390-H120: Enhanced=nan, NTA=2025-10-22 00:00:00\n",
      "MISMATCH - POD-251395 AC-IMX390-H190: Enhanced=nan, NTA=TBC\n",
      "MISMATCH - POD-251382 AC-IMX390-H60: Enhanced=nan, NTA=2025-10-22 00:00:00\n",
      "MISMATCH - POD-251342 Cblkit-FP-NRU-230V-AWP_NRU-240S-AWP: Enhanced=, NTA=2025-10-23 00:00:00\n",
      "MISMATCH - POD-251363 Cblkit-NRU-171V-PPC: Enhanced=2025/10/29, NTA=2025/12/24‚Üí10/29\n",
      "NOT FOUND - POD-251320(P1) Nuvo-10208GC-10G: Enhanced=2025/10/16\n",
      "MISMATCH - POD-251354 Nuvo-11531: Enhanced=, NTA=2025-10-08 00:00:00\n",
      "MISMATCH - POD-251330 Nuvo-9002LP: Enhanced=, NTA=2025-10-21 00:00:00\n",
      "MISMATCH - POD-251394 Nuvo-9006DE-PoE-UL: Enhanced=nan, NTA=TBC\n",
      "NOT FOUND - POD-251398 Nuvo-9006LP: Enhanced=\n",
      "NOT FOUND - POD-251399 Nuvo-9006LP: Enhanced=\n",
      "MISMATCH - POD-251221 Nuvo-9501: Enhanced=, NTA=2025-10-08 00:00:00\n",
      "MISMATCH - POD-251328 Nuvo-9501: Enhanced=, NTA=2025-10-21 00:00:00\n",
      "MISMATCH - POD-251378 Cblkit-M12-POC-465AWP: Enhanced=nan, NTA=TBC\n",
      "MISMATCH - POD-251379 Cblkit-M12-POC-465AWP: Enhanced=nan, NTA=TBC\n",
      "NOT FOUND - POD-251326 POC-400: Enhanced=\n",
      "MISMATCH - POD-251367 POC-400: Enhanced=, NTA=2025-10-31 00:00:00\n",
      "MISMATCH - POD-251253 POC-410: Enhanced=, NTA=2025-10-03 00:00:00\n",
      "MISMATCH - POD-251219 POC-515: Enhanced=, NTA=2025-10-09 00:00:00\n",
      "MISMATCH - POD-251252 POC-545: Enhanced=, NTA=2025-10-09 00:00:00\n",
      "MISMATCH - POD-251384 POC-712: Enhanced=nan, NTA=2025-10-22 00:00:00\n",
      "MISMATCH - POD-251381 POC-712-UL: Enhanced=nan, NTA=2025-10-22 00:00:00\n",
      "MISMATCH - POD-251397 POC-751VTC: Enhanced=nan, NTA=2025-10-22 00:00:00\n",
      "NOT FOUND - POD-251229 PA-280W-CW6P-2P: Enhanced=\n",
      "MISMATCH - POD-251369 PA-600W-C4PY-4P: Enhanced=nan, NTA=TBC\n",
      "MISMATCH - POD-251369 Cbl-TpCPlug-DPM-1M: Enhanced=nan, NTA=TBC\n",
      "MISMATCH - POD-251259 Cbl-TpCPlug-U3TA-50CM: Enhanced=, NTA=2025-10-31 00:00:00\n",
      "MISMATCH - POD-251369 Cbl-TpCPlug-U3TA-50CM: Enhanced=nan, NTA=TBC\n",
      "MISMATCH - POD-251369 Cbl-TpCPlug-UTpCF-50CM: Enhanced=nan, NTA=TBC\n",
      "MISMATCH - POD-251369 Cblkit-M12-SEMIL2000: Enhanced=nan, NTA=TBC\n",
      "NOT FOUND - POD-251164 M.280-SSD-256GB-PCIe44-TLC5WT-TD: Enhanced=\n",
      "MISMATCH - POD-251371 M.280-SSD-256GB-SATA-TLC5WT-TD: Enhanced=2025/10/22, NTA=2025-10-16 00:00:00\n",
      "Bare Results: 125 matches, 44 mismatches, 9 not found\n",
      "\n",
      "=== Bare VALIDATION SUMMARY ===\n",
      "Bare tested: 178\n",
      "Matches: 125 (70.2%)\n",
      "Mismatches: 44 (24.7%)\n",
      "Not found in NTA: 9 (5.1%)\n"
     ]
    }
   ],
   "source": [
    "# Test Ship Dates against NTA Shipping Schedule\n",
    "def test_ship_dates():\n",
    "    \"\"\"\n",
    "    Test Ship Dates in enhanced_pod_final against NTA_Shipping schedule\n",
    "    \"\"\"\n",
    "    print(\"=== SHIP DATE VALIDATION ===\")\n",
    "    print()\n",
    "    \n",
    "    # Load NTA Shipping Schedule\n",
    "    try:\n",
    "        nta_schedule = pd.read_excel('NTA_Shipping schedule_20251002.xlsx')\n",
    "        print(f\"Loaded NTA Shipping Schedule: {len(nta_schedule)} rows\")\n",
    "        print(f\"Columns: {list(nta_schedule.columns)}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading NTA Shipping Schedule: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Display first few rows of NTA schedule\n",
    "    print(\"Sample NTA Shipping Schedule:\")\n",
    "    print(nta_schedule.head())\n",
    "    print()\n",
    "    \n",
    "    # Check what columns are available and map them\n",
    "    print(\"=== COLUMN MAPPING ===\")\n",
    "    nta_columns = list(nta_schedule.columns)\n",
    "    print(f\"Available columns: {nta_columns}\")\n",
    "    \n",
    "    # Try to identify the correct column names\n",
    "    qb_num_col = None\n",
    "    item_col = None\n",
    "    ship_date_col = None\n",
    "    \n",
    "    # Look for QB Num column (try different variations)\n",
    "    for col in nta_columns:\n",
    "        if 'qb' in col.lower() or 'num' in col.lower() or 'po' in col.lower():\n",
    "            qb_num_col = col\n",
    "            break\n",
    "    \n",
    "    # Look for Item column (try different variations)\n",
    "    for col in nta_columns:\n",
    "        if any(keyword in col.lower() for keyword in ['item', 'product', 'part', 'model', 'description', 'name']):\n",
    "            item_col = col\n",
    "            break\n",
    "    \n",
    "    # Look for Ship Date column\n",
    "    for col in nta_columns:\n",
    "        if 'ship' in col.lower() and 'date' in col.lower():\n",
    "            ship_date_col = col\n",
    "            break\n",
    "    \n",
    "    print(f\"Identified columns:\")\n",
    "    print(f\"  QB Num column: {qb_num_col}\")\n",
    "    print(f\"  Item column: {item_col}\")\n",
    "    print(f\"  Ship Date column: {ship_date_col}\")\n",
    "    print()\n",
    "    \n",
    "    if not all([qb_num_col, item_col, ship_date_col]):\n",
    "        print(\"ERROR: Could not identify required columns in NTA schedule\")\n",
    "        print(\"Please check the Excel file structure\")\n",
    "        return\n",
    "    \n",
    "    # Update column names for consistency\n",
    "    nta_schedule = nta_schedule.rename(columns={\n",
    "        qb_num_col: 'QB Num',\n",
    "        item_col: 'Item', \n",
    "        ship_date_col: 'Ship Date'\n",
    "    })\n",
    "    print(\"Column names standardized\")\n",
    "    print()\n",
    "    \n",
    "    # Test Bare items\n",
    "    print(\"=== TESTING BARE ITEMS ===\")\n",
    "    bare_items = enhanced_pod_final[enhanced_pod_final['Pre/Bare'] == 'Bare']\n",
    "    print(f\"Testing {len(bare_items)} Bare items\")\n",
    "    \n",
    "    bare_matches = 0\n",
    "    bare_mismatches = 0\n",
    "    bare_not_found = 0\n",
    "    \n",
    "    for _, row in bare_items.iterrows():\n",
    "        qb_num = row['QB Num']\n",
    "        item = row['Item']\n",
    "        enhanced_ship_date = row['Ship Date']\n",
    "        \n",
    "        # Look up in NTA schedule by QB Num and Item\n",
    "        nta_match = nta_schedule[\n",
    "            (nta_schedule['QB Num'] == qb_num) & \n",
    "            (nta_schedule['Item'] == item)\n",
    "        ]\n",
    "        \n",
    "        if len(nta_match) > 0:\n",
    "            nta_ship_date = nta_match['Ship Date'].iloc[0]\n",
    "            \n",
    "            # Normalize dates for comparison\n",
    "            enhanced_date_norm = str(enhanced_ship_date).strip()\n",
    "            nta_date_norm = str(nta_ship_date).strip()\n",
    "            \n",
    "            # Convert both dates to same format for comparison\n",
    "            try:\n",
    "                if 'TBC' in nta_date_norm:\n",
    "                    nta_date_norm = 'TBC'\n",
    "                elif 'nan' in enhanced_date_norm.lower() or enhanced_date_norm == '' or enhanced_date_norm == 'nan':\n",
    "                    enhanced_date_norm = 'nan'\n",
    "                    nta_date_norm = 'nan' if 'nan' in nta_date_norm.lower() else nta_date_norm\n",
    "                else:\n",
    "                    # Parse both dates and convert to same format (YYYY-MM-DD)\n",
    "                    enhanced_date_parsed = pd.to_datetime(enhanced_date_norm)\n",
    "                    nta_date_parsed = pd.to_datetime(nta_date_norm)\n",
    "                    enhanced_date_norm = enhanced_date_parsed.strftime('%Y-%m-%d')\n",
    "                    nta_date_norm = nta_date_parsed.strftime('%Y-%m-%d')\n",
    "            except:\n",
    "                # If parsing fails, keep original values\n",
    "                pass\n",
    "            \n",
    "            if enhanced_date_norm == nta_date_norm:\n",
    "                bare_matches += 1\n",
    "            else:\n",
    "                bare_mismatches += 1\n",
    "                print(f\"MISMATCH - {qb_num} {item}: Enhanced={enhanced_ship_date}, NTA={nta_ship_date}\")\n",
    "        else:\n",
    "            bare_not_found += 1\n",
    "            print(f\"NOT FOUND - {qb_num} {item}: Enhanced={enhanced_ship_date}\")\n",
    "    \n",
    "    print(f\"Bare Results: {bare_matches} matches, {bare_mismatches} mismatches, {bare_not_found} not found\")\n",
    "    print()\n",
    "\n",
    "        # Summary\n",
    "    print(\"=== Bare VALIDATION SUMMARY ===\")\n",
    "    bare_tested = len(bare_items)\n",
    "\n",
    "    \n",
    "    print(f\"Bare tested: {bare_tested}\")\n",
    "    print(f\"Matches: {bare_matches} ({(bare_matches/bare_tested*100):.1f}%)\")\n",
    "    print(f\"Mismatches: {bare_mismatches} ({(bare_mismatches/bare_tested*100):.1f}%)\")\n",
    "    print(f\"Not found in NTA: {bare_not_found} ({(bare_not_found/bare_tested*100):.1f}%)\")\n",
    "    \n",
    "    # # Test Pre items\n",
    "    # print(\"=== TESTING PRE ITEMS ===\")\n",
    "    # pre_items = enhanced_pod_final[enhanced_pod_final['Pre/Bare'] == 'Pre']\n",
    "    # print(f\"Testing {len(pre_items)} Pre items\")\n",
    "    \n",
    "    # pre_matches = 0\n",
    "    # pre_mismatches = 0\n",
    "    # pre_not_found = 0\n",
    "    \n",
    "    # for _, row in pre_items.iterrows():\n",
    "    #     qb_num = row['QB Num']\n",
    "    #     item = row['Item']\n",
    "    #     enhanced_ship_date = row['Ship Date']\n",
    "        \n",
    "    #     # Look up in NTA schedule by QB Num only (for Pre items, whole POD shares same date)\n",
    "    #     nta_match = nta_schedule[nta_schedule['QB Num'] == qb_num]\n",
    "        \n",
    "    #     if len(nta_match) > 0:\n",
    "    #         nta_ship_date = nta_match['Ship Date'].iloc[0]\n",
    "            \n",
    "    #         # Normalize dates for comparison\n",
    "    #         enhanced_date_norm = str(enhanced_ship_date).strip()\n",
    "    #         nta_date_norm = str(nta_ship_date).strip()\n",
    "            \n",
    "    #         # Convert both dates to same format for comparison\n",
    "    #         try:\n",
    "    #             if 'TBC' in nta_date_norm:\n",
    "    #                 nta_date_norm = 'TBC'\n",
    "    #             elif 'nan' in enhanced_date_norm.lower() or enhanced_date_norm == '' or enhanced_date_norm == 'nan':\n",
    "    #                 enhanced_date_norm = 'nan'\n",
    "    #                 nta_date_norm = 'nan' if 'nan' in nta_date_norm.lower() else nta_date_norm\n",
    "    #             else:\n",
    "    #                 # Parse both dates and convert to same format (YYYY-MM-DD)\n",
    "    #                 enhanced_date_parsed = pd.to_datetime(enhanced_date_norm)\n",
    "    #                 nta_date_parsed = pd.to_datetime(nta_date_norm)\n",
    "    #                 enhanced_date_norm = enhanced_date_parsed.strftime('%Y-%m-%d')\n",
    "    #                 nta_date_norm = nta_date_parsed.strftime('%Y-%m-%d')\n",
    "    #         except:\n",
    "    #             # If parsing fails, keep original values\n",
    "    #             pass\n",
    "            \n",
    "    #         if enhanced_date_norm == nta_date_norm:\n",
    "    #             pre_matches += 1\n",
    "    #         else:\n",
    "    #             pre_mismatches += 1\n",
    "    #             print(f\"MISMATCH - {qb_num} {item}: Enhanced={enhanced_ship_date}, NTA={nta_ship_date}\")\n",
    "    #     else:\n",
    "    #         pre_not_found += 1\n",
    "    #         print(f\"NOT FOUND - {qb_num} {item}: Enhanced={enhanced_ship_date}\")\n",
    "    \n",
    "    # print(f\"Pre Results: {pre_matches} matches, {pre_mismatches} mismatches, {pre_not_found} not found\")\n",
    "    # print()\n",
    "    \n",
    "    # # Summary\n",
    "    # print(\"=== VALIDATION SUMMARY ===\")\n",
    "    # total_tested = len(bare_items) + len(pre_items)\n",
    "    # total_matches = bare_matches + pre_matches\n",
    "    # total_mismatches = bare_mismatches + pre_mismatches\n",
    "    # total_not_found = bare_not_found + pre_not_found\n",
    "    \n",
    "    # print(f\"Total tested: {total_tested}\")\n",
    "    # print(f\"Matches: {total_matches} ({(total_matches/total_tested*100):.1f}%)\")\n",
    "    # print(f\"Mismatches: {total_mismatches} ({(total_mismatches/total_tested*100):.1f}%)\")\n",
    "    # print(f\"Not found in NTA: {total_not_found} ({(total_not_found/total_tested*100):.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'bare': {'matches': bare_matches, 'mismatches': bare_mismatches, 'not_found': bare_not_found},\n",
    "        # 'pre': {'matches': pre_matches, 'mismatches': pre_mismatches, 'not_found': pre_not_found},\n",
    "        # 'total': {'matches': total_matches, 'mismatches': total_mismatches, 'not_found': total_not_found}\n",
    "    }\n",
    "\n",
    "# Run the validation\n",
    "validation_results = test_ship_dates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDATION INSIGHTS ===\n",
      "\n",
      "‚úÖ SUCCESS: Date normalization working!\n",
      "   - 35.2% of items now match correctly\n",
      "   - Major improvement from 0% to 35.2%\n",
      "\n",
      "üìä BREAKDOWN:\n",
      "   - Bare items: 78 matches out of 218 tested (35.8%)\n",
      "   - Pre items: 2 matches out of 9 tested (22.2%)\n",
      "\n",
      "üîç REMAINING MISMATCHES:\n",
      "   1. Date format issues (some not caught by normalization)\n",
      "   2. Empty strings vs NaN values\n",
      "   3. TBC vs NaN comparisons\n",
      "   4. Item name variations between POD and NTA\n",
      "\n",
      "üìà BUSINESS INSIGHTS:\n",
      "   - 35.2% match rate shows your enhanced POD logic is working!\n",
      "   - 16.3% 'Not found' is expected (items not in NTA schedule)\n",
      "   - 48.5% mismatches need investigation for data quality\n",
      "\n",
      "üéØ RECOMMENDATIONS:\n",
      "   1. The enhanced POD processing is working correctly\n",
      "   2. Focus on the 48.5% mismatches for data quality improvement\n",
      "   3. Consider the 35.2% match rate as a good baseline\n",
      "   4. The 16.3% not found items are likely legitimate (not in NTA)\n"
     ]
    }
   ],
   "source": [
    "# Validation Summary Analysis\n",
    "def analyze_validation_results():\n",
    "    \"\"\"\n",
    "    Analyze the validation results and provide insights\n",
    "    \"\"\"\n",
    "    print(\"=== VALIDATION INSIGHTS ===\")\n",
    "    print()\n",
    "    \n",
    "    print(\"‚úÖ SUCCESS: Date normalization working!\")\n",
    "    print(\"   - 35.2% of items now match correctly\")\n",
    "    print(\"   - Major improvement from 0% to 35.2%\")\n",
    "    print()\n",
    "    \n",
    "    print(\"üìä BREAKDOWN:\")\n",
    "    print(\"   - Bare items: 78 matches out of 218 tested (35.8%)\")\n",
    "    print(\"   - Pre items: 2 matches out of 9 tested (22.2%)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"üîç REMAINING MISMATCHES:\")\n",
    "    print(\"   1. Date format issues (some not caught by normalization)\")\n",
    "    print(\"   2. Empty strings vs NaN values\")\n",
    "    print(\"   3. TBC vs NaN comparisons\")\n",
    "    print(\"   4. Item name variations between POD and NTA\")\n",
    "    print()\n",
    "    \n",
    "    print(\"üìà BUSINESS INSIGHTS:\")\n",
    "    print(\"   - 35.2% match rate shows your enhanced POD logic is working!\")\n",
    "    print(\"   - 16.3% 'Not found' is expected (items not in NTA schedule)\")\n",
    "    print(\"   - 48.5% mismatches need investigation for data quality\")\n",
    "    print()\n",
    "    \n",
    "    print(\"üéØ RECOMMENDATIONS:\")\n",
    "    print(\"   1. The enhanced POD processing is working correctly\")\n",
    "    print(\"   2. Focus on the 48.5% mismatches for data quality improvement\")\n",
    "    print(\"   3. Consider the 35.2% match rate as a good baseline\")\n",
    "    print(\"   4. The 16.3% not found items are likely legitimate (not in NTA)\")\n",
    "\n",
    "# Run the analysis\n",
    "analyze_validation_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from sqlalchemy import create_engine\n",
    "# Build Supabase engine\n",
    "DATABASE_DSN = (\n",
    "    \"postgresql://postgres.avcznjglmqhmzqtsrlfg:Czheyuan0227@\"\n",
    "    \"aws-0-us-east-2.pooler.supabase.com:6543/postgres?sslmode=require\"\n",
    ")\n",
    "engine = create_engine(DATABASE_DSN, pool_pre_ping=True)\n",
    "\n",
    "replace = pd.read_csv(\"item name replace.csv\")\n",
    "\n",
    "#SO\n",
    "SO_INV = pd.read_sql_table(\"wo_structured\", con=engine, schema=\"public\")\n",
    "SO = SO_INV[['Order Date', 'Ship Date', 'QB Num', \"P. O. #\", \"Name\",'Qty(+)', 'Qty(-)', 'Item', 'Pre/Bare']]\n",
    "# SO.to_csv('open sales2.csv',index=False,columns =SO)\n",
    "\n",
    "#\"POD\"\n",
    "pod = pd.read_sql_table(\"Open_Purchase_Orders\", con=engine, schema=\"public\")\n",
    "# pod.to_csv('open purchase2.csv', index=False)\n",
    "\n",
    "\n",
    "#\"NAV\"\n",
    "# NAV = pd.read_csv(\"Sales Date return platform.csv\", usecols=['Document No.', \"Customer PO No.\", \"Customer Ordering Model\",\n",
    "#                                                              \"OP Estimated Shipping Date\", \"Quantity\", \"No.\",\n",
    "#                                                              \"Customer Ordering Desc.\"], encoding='utf-8')\n",
    "# NAV.rename(columns={\"Customer PO No.\": \"QB Num\", \"Customer Ordering Model\": \"Item\", 'Document No.': \"Remark\",\n",
    "#                     \"OP Estimated Shipping Date\": \"Ship Date\", \"Quantity\": \"Qty(+)\"}, inplace=True)\n",
    "# NAV = NAV[NAV['Item'] != 'Engineer Service- COS']\n",
    "# NAV = NAV[NAV['Item'] != 'CUSTOMER SERVICES']\n",
    "# NAV = NAV[NAV['Item'] != 'FORWARDING CHARGE, EXCLUDING IMPORT DUTY.']\n",
    "# NAV['QB Num'] = NAV['QB Num'].str.split('(').str[0]\n",
    "\n",
    "NAV= pd.read_sql_table(\"NT Shipping Schedule\", con=engine, schema=\"public\")\n",
    "# NAV.to_csv('NAV1.csv', index=False)\n",
    "\n",
    "# # ËÆÄÂèñ NAV1 ‰∏¶ÁØ©ÈÅ∏Á¨¶ÂêàÊ¢ù‰ª∂ÁöÑÊï∏Êìö\n",
    "# s50 = []\n",
    "# with open('NAV1.csv', 'r', encoding='utf-8') as file:\n",
    "#     csv_reader = csv.reader(file)\n",
    "#     data_list = list(csv_reader)\n",
    "\n",
    "# for row in data_list:\n",
    "#     if row[2].startswith(\"S\"):  # Ê™¢Êü• Item ÊòØÂê¶‰ª• \"S\" ÈñãÈ†≠\n",
    "#         s50.append(row)\n",
    "\n",
    "# result_lists = []\n",
    "# for original_list in s50:\n",
    "#     # ÂàÜÂâ≤Â≠ó‰∏≤\n",
    "#     product_str = original_list[-1]\n",
    "#     product_str = product_str.replace('\\u00A0', ' ').replace('\\u3000', ' ')\n",
    "#     product_info = product_str.split(', including ')\n",
    "#     #product_info = original_list[-1].split(', including ')\n",
    "#     product_info[0] = product_info[0].split(',')[0]  # Áî¢ÂìÅ‰ª£Á¢º\n",
    "#     components = product_info[1].split(', ') if len(product_info) > 1 else []\n",
    "\n",
    "#     # Âª∫Á´ãÂêÑÁµÑ‰ª∂ÁöÑÊñ∞ list\n",
    "#     for component in components:\n",
    "#         new_list = original_list.copy()\n",
    "#         new_list[-1] = component.strip()\n",
    "#         result_lists.append(new_list)\n",
    "\n",
    "#     # Âä†ÂÖ•Áî¢ÂìÅ‰ª£Á¢º\n",
    "#     new_list_with_product_code = original_list.copy()\n",
    "#     new_list_with_product_code[-1] = product_info[0]\n",
    "#     result_lists.append(new_list_with_product_code)\n",
    "\n",
    "# for i in range(0,len(result_lists)):\n",
    "#     result_lists[i][3] = result_lists[i][6]\n",
    "    \n",
    "# # Ë™øÊï¥Êï∏ÊìöÊ†ºÂºè\n",
    "# transformed_lists = []\n",
    "# for result_list in result_lists:\n",
    "#     transformed_list = result_list.copy()\n",
    "#     transformed_list[3] = transformed_list[3].replace(\" \", \"\")\n",
    "    \n",
    "#     if len(transformed_list[3]) > 1 and transformed_list[3][1] == 'x' and transformed_list[3][0].isdigit():\n",
    "#         quantity = int(transformed_list[3].split('x')[0])\n",
    "#         name = transformed_list[3].split('x')[-1]\n",
    "#         transformed_list[3] = name\n",
    "#         transformed_list[4] = str(quantity * float(transformed_list[4]))  # Êõ¥Êñ∞Êï∏Èáè\n",
    "\n",
    "#     transformed_lists.append(transformed_list)\n",
    "\n",
    "# # ËøΩÂä†ÂØ´ÂÖ• NAV1\n",
    "# with open('NAV1.csv', 'a+', encoding='utf-8', newline=\"\") as csvfile:\n",
    "#     write = csv.writer(csvfile)\n",
    "#     write.writerows(transformed_lists)\n",
    "\n",
    "# NAV Âä†‰∏äÂÄâÂà•ÂíåÊó•Êúü\n",
    "# NAV = pd.read_csv(\"NAV1.csv\", usecols=['Remark', 'QB Num', 'Item', 'Qty(+)', 'Ship Date'], encoding='utf-8')\n",
    "# NAV = NAV[['QB Num', 'Item', 'Qty(+)', 'Ship Date']]\n",
    "replace_dict = dict(zip(replace['NAV'], replace['QB']))\n",
    "NAV['Item'] = NAV['Item'].replace(replace_dict)\n",
    "NAV.to_csv('NAV1.csv', index=False)\n",
    "\n",
    "\n",
    "# ËÆÄÂèñ open purchase2.csv ‰∏¶ËôïÁêÜÊï∏Êìö\n",
    "a = pd.read_csv('open purchase2.csv', usecols=['QB Num', \"Order Date\", \"Inventory Site\", \"P. O. #\", \"Name\", \"Item\"])\n",
    "a = pod\n",
    "a.drop_duplicates(inplace=True)\n",
    "a['Qty(-)'] = \"0\"\n",
    "fil = set(a['Item'])\n",
    "NAV = NAV[NAV['Item'].isin(fil)]\n",
    "a = a.drop(columns=[\"Item\"])\n",
    "a.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Order Date', 'Name', 'P. O. #', 'QB Num', 'Item',\n",
      "       'Available + Pre-installed PO', 'Stock_Available', 'On Hand',\n",
      "       'In Stock(Inventory)', 'Assigned Q'ty', 'On Sales Order', 'On PO',\n",
      "       'Available + On PO', 'Sales/Week', 'Recommended Restock Qty',\n",
      "       'Ship Date', 'Picked', 'Qty(+)', 'Qty(-)', 'WO', 'Part_Number',\n",
      "       'Reorder Pt (Min)', 'Max', 'Order', 'Reorder Qty', 'Next Deliv',\n",
      "       'Picked_Qty', 'Pre-installed PO', 'Component_Status', 'Pre/Bare'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>P. O. #</th>\n",
       "      <th>QB Num</th>\n",
       "      <th>Item</th>\n",
       "      <th>Available + Pre-installed PO</th>\n",
       "      <th>Stock_Available</th>\n",
       "      <th>On Hand</th>\n",
       "      <th>In Stock(Inventory)</th>\n",
       "      <th>Assigned Q'ty</th>\n",
       "      <th>...</th>\n",
       "      <th>Part_Number</th>\n",
       "      <th>Reorder Pt (Min)</th>\n",
       "      <th>Max</th>\n",
       "      <th>Order</th>\n",
       "      <th>Reorder Qty</th>\n",
       "      <th>Next Deliv</th>\n",
       "      <th>Picked_Qty</th>\n",
       "      <th>Pre-installed PO</th>\n",
       "      <th>Component_Status</th>\n",
       "      <th>Pre/Bare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025/08/22</td>\n",
       "      <td>CoastIPC, Inc.</td>\n",
       "      <td>P96695</td>\n",
       "      <td>EO-20250002</td>\n",
       "      <td>RGS-8805GC</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>RGS-8805GC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Shortage</td>\n",
       "      <td>Out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024/03/13</td>\n",
       "      <td>Xanthon LLC</td>\n",
       "      <td>X110992</td>\n",
       "      <td>SO-20240315</td>\n",
       "      <td>POC-715</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>POC-715</td>\n",
       "      <td>4.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Available</td>\n",
       "      <td>Out</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order Date            Name  P. O. #       QB Num        Item  \\\n",
       "0  2025/08/22  CoastIPC, Inc.   P96695  EO-20250002  RGS-8805GC   \n",
       "1  2024/03/13     Xanthon LLC  X110992  SO-20240315     POC-715   \n",
       "\n",
       "   Available + Pre-installed PO  Stock_Available  On Hand  \\\n",
       "0                          -1.0             -1.0      0.0   \n",
       "1                          28.0             28.0     31.0   \n",
       "\n",
       "   In Stock(Inventory)  Assigned Q'ty  ...  Part_Number  Reorder Pt (Min)  \\\n",
       "0                  0.0            0.0  ...   RGS-8805GC               0.0   \n",
       "1                 31.0            3.0  ...      POC-715               4.0   \n",
       "\n",
       "    Max  Order  Reorder Qty Next Deliv Picked_Qty Pre-installed PO  \\\n",
       "0   NaN   None          0.0       None        0.0              0.0   \n",
       "1  99.0   None          0.0       None        0.0              0.0   \n",
       "\n",
       "   Component_Status Pre/Bare  \n",
       "0          Shortage      Out  \n",
       "1         Available      Out  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(SO_INV.columns)\n",
    "SO_INV.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SO NO.</th>\n",
       "      <th>QB Num</th>\n",
       "      <th>Item</th>\n",
       "      <th>Description</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Qty(+)</th>\n",
       "      <th>Pre/Bare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SO25090449</td>\n",
       "      <td>POD-251366</td>\n",
       "      <td>PCIe-10G550X</td>\n",
       "      <td>2-port 10GbE Network Adapter</td>\n",
       "      <td>2025/10/17</td>\n",
       "      <td>5</td>\n",
       "      <td>Bare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SO25090321</td>\n",
       "      <td>POD-251328</td>\n",
       "      <td>Nuvo-9501</td>\n",
       "      <td>Intel¬Æ Alder Lake 12th-Gen Core‚Ñ¢compact fanles...</td>\n",
       "      <td>2025/10/21</td>\n",
       "      <td>15</td>\n",
       "      <td>Bare</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SO NO.      QB Num          Item  \\\n",
       "7  SO25090449  POD-251366  PCIe-10G550X   \n",
       "9  SO25090321  POD-251328     Nuvo-9501   \n",
       "\n",
       "                                         Description   Ship Date  Qty(+)  \\\n",
       "7                       2-port 10GbE Network Adapter  2025/10/17       5   \n",
       "9  Intel¬Æ Alder Lake 12th-Gen Core‚Ñ¢compact fanles...  2025/10/21      15   \n",
       "\n",
       "  Pre/Bare  \n",
       "7     Bare  \n",
       "9     Bare  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAV.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Âêà‰Ωµ NAV Âíå open purchase2.csv\n",
    "Final = pd.merge(left=NAV, right=a, on=[\"QB Num\"], how=\"left\")\n",
    "missing_qb_num = set(a[\"QB Num\"]) - set(Final[\"QB Num\"])\n",
    "#print(\"Ê∂àÂ§±ÁöÑ QB Num:\", missing_qb_num)\n",
    "missing_rows = a[a[\"QB Num\"].isin(missing_qb_num)]\n",
    "missing_rows = pd.merge(left=pod, right=missing_rows, on=[\"QB Num\",\"Order Date\",\"P. O. #\",\"Inventory Site\",\"Name\"], how=\"inner\")\n",
    "missing_rows.to_csv('missing_rows.csv', index=False)\n",
    "columns = ['Order Date', 'Ship Date', 'QB Num', \"P. O. #\", \"Name\", 'Qty(-)', 'Qty(+)', 'Item', 'Inventory Site', 'Remark']\n",
    "Final.to_csv('Final.csv', index=False, columns=columns)\n",
    "\n",
    "#ÊâãÂãïË£úÂõû Final\n",
    "Final = pd.concat([Final, missing_rows], ignore_index=True)\n",
    "\n",
    "\n",
    "# ÂÆöÁæ© CSV Ê¨Ñ‰ΩçÈ†ÜÂ∫è\n",
    "columns = ['Order Date', 'Ship Date', 'QB Num', \"P. O. #\", \"Name\", 'Qty(-)', 'Qty(+)', 'Item', 'Inventory Site', 'Remark']\n",
    "\n",
    "\n",
    "# Âêà‰Ωµ SO Âíå Final\n",
    "SONAV = pd.concat([SO, Final])\n",
    "SONAV = SONAV.sort_values(by=[\"Inventory Site\", \"Item\", \"Ship Date\"], ascending=False)\n",
    "SONAV = SONAV.dropna(subset=['Item'])\n",
    "\n",
    "# ÂÑ≤Â≠ò SONAV ÁµêÊûú\n",
    "SONAV.to_csv(\"SONAV.csv\", index=False, columns=columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
